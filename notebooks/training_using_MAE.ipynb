{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T12:46:44.088688Z",
     "iopub.status.busy": "2024-03-27T12:46:44.088377Z",
     "iopub.status.idle": "2024-03-27T12:46:57.265110Z",
     "shell.execute_reply": "2024-03-27T12:46:57.263672Z",
     "shell.execute_reply.started": "2024-03-27T12:46:44.088664Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting einops\n",
      "  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: einops\n",
      "Successfully installed einops-0.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install einops\n",
    "!git clone https://github.com/b-ptiste/dlmi.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T12:46:57.267074Z",
     "iopub.status.busy": "2024-03-27T12:46:57.266742Z",
     "iopub.status.idle": "2024-03-27T12:47:13.140701Z",
     "shell.execute_reply": "2024-03-27T12:47:13.139730Z",
     "shell.execute_reply.started": "2024-03-27T12:46:57.267045Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'dlmi'...\n",
      "remote: Enumerating objects: 227, done.\u001b[K\n",
      "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
      "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
      "remote: Total 227 (delta 4), reused 3 (delta 0), pack-reused 210\u001b[K\n",
      "Receiving objects: 100% (227/227), 45.05 KiB | 490.00 KiB/s, done.\n",
      "Resolving deltas: 100% (84/84), done.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "\n",
    "# Related third-party imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms as T\n",
    "from torchvision.transforms import v2\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "import wandb\n",
    "import uuid\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "# log in different framework\n",
    "path_root = \"/kaggle/input/dlmi-challenge-b-and-s\"\n",
    "path_working = \"/kaggle/working\"\n",
    "path_mae = \"/kaggle/input/mae-pretrain\"\n",
    "\n",
    "\n",
    "# local library\n",
    "from dlmi.src.model import ModelFactory\n",
    "from dlmi.src.data import csv_processing, DataloaderFactory\n",
    "from dlmi.src.utils import get_stratified_split\n",
    "from dlmi.src.mae_pretraining import MAE_ViT, MAE_Encoder, MAE_Decoder, PatchShuffle\n",
    "from dlmi.data.split import train_index as train_index_strat\n",
    "from dlmi.data.split import val_index as val_index_strat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create config.\n",
    "\n",
    "This config contains all the hyparameter usefull for our experiments. There will be logged in wandb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight for the MAE pretraining are avalaible and need to be download here : \n",
    "\n",
    "Drive with the weigth [here](https://drive.google.com/drive/u/0/folders/13yrd36hwnCahIzXtedJdakCQZdADHxLd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T12:47:13.144677Z",
     "iopub.status.busy": "2024-03-27T12:47:13.143905Z",
     "iopub.status.idle": "2024-03-27T12:47:13.153445Z",
     "shell.execute_reply": "2024-03-27T12:47:13.152500Z",
     "shell.execute_reply.started": "2024-03-27T12:47:13.144640Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"who\": \"baptiste\",  # or steven\n",
    "    \"no_wandb\": False,\n",
    "    \"name_exp\": \"PatientModelCrossAttentionTab - vit_small_patch16_224 - lora\",\n",
    "    \"lr\": 5e-6,\n",
    "    \"batch_size\": 1,\n",
    "    \"nb_epochs\": 20,\n",
    "    \"timm\": True,  # is the model from timm\n",
    "    \"timm_model\": \"vit_small_patch16_224.augreg_in21k\",\n",
    "    \"dino\": False,\n",
    "    \"dino_size\": \"vits\",  # vits, vitb, vitl, vitg\n",
    "    \"adapter\": \"lora\",  # bottleneck, adaptformer, lora, prompttuning\n",
    "    \"model_name\": \"PatientModelCrossAttentionTab\",  # 'vit_small_patch16_224.augreg_in21k', #timm based model\n",
    "    \"pretrained\": True,\n",
    "    \"pretrained_path\": \"\",\n",
    "    \"nb_class\": 2,\n",
    "    \"scheduler\": None,  # could be empty or linear, expo ...\n",
    "    \"dataset_name\": \"DatasetPerPatient\",\n",
    "    \"device_1\": \"cuda:0\",\n",
    "    \"device_2\": \"cuda:1\",  # for double device\n",
    "    # data augmentation\n",
    "    \"filename\": f\"{path_working}/submission.csv\",\n",
    "    \"sub_batch_size\": 16,\n",
    "    \"latent_att\": 512,\n",
    "    \"head_1\": 8,  # 4\n",
    "    \"head_2\": 2,\n",
    "    \"feature_dim\": 384,  # DINOv2, VIT: 192 - 384\n",
    "    \"aggregation\": \"avg\",  # sum, avg, max\n",
    "    \"beta_1\": 0.5,\n",
    "    \"beta_2\": 0.9,\n",
    "    \"weight_decay\": 5e-2,\n",
    "    \"weight_class_0\": 3.0,\n",
    "    \"weight_class_1\": 1.0,\n",
    "    \"mask_ratio\": 0.75,\n",
    "    \"image_size\": 224,\n",
    "    \"patch_size\": 16,\n",
    "    \"mae_pretrained\": \"small_testset_800it.pt\",\n",
    "    \"with_tab\": True,\n",
    "    \"mode_split\": \"strat\",  # load, strat\n",
    "    \"degrees\": (-5, 5),\n",
    "    \"translate\": (0.1, 0.1),\n",
    "    \"scale\": (0.9, 1.0),\n",
    "    \"fill\": (255, 232, 201),\n",
    "    \"p\": 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data importation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Csv import with pre-processing, reformatting and normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T12:47:13.154974Z",
     "iopub.status.busy": "2024-03-27T12:47:13.154675Z",
     "iopub.status.idle": "2024-03-27T12:47:13.205727Z",
     "shell.execute_reply": "2024-03-27T12:47:13.204997Z",
     "shell.execute_reply.started": "2024-03-27T12:47:13.154934Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_annotation_train = csv_processing(\n",
    "    os.path.join(path_root, \"trainset\", \"trainset_true.csv\")\n",
    ")\n",
    "df_annotation_test = csv_processing(\n",
    "    os.path.join(path_root, \"testset\", \"testset_data.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train, val, test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T12:47:13.207019Z",
     "iopub.status.busy": "2024-03-27T12:47:13.206737Z",
     "iopub.status.idle": "2024-03-27T12:47:13.213851Z",
     "shell.execute_reply": "2024-03-27T12:47:13.212888Z",
     "shell.execute_reply.started": "2024-03-27T12:47:13.206994Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_index = df_annotation_test.index.tolist()\n",
    "\n",
    "if cfg[\"mode_split\"] == \"auto\":\n",
    "    map_mode_index = get_stratified_split(df_annotation_train, df_annotation_test)\n",
    "\n",
    "    train_index = map_mode_index[\"train\"]\n",
    "    val_index = map_mode_index[\"val\"]\n",
    "\n",
    "elif cfg[\"mode_split\"] == \"load\":\n",
    "    # log wandb\n",
    "    run = wandb.init()\n",
    "    artifact = run.use_artifact(\n",
    "        \"ii_timm/DLMI/submission958f5028e70811ee9d6b0242ac130202:v0\", type=\"csv\"\n",
    "    )\n",
    "    artifact_dir = artifact.download(root=path_working)\n",
    "    wandb.finish()\n",
    "\n",
    "    train_index = pd.read_csv(f\"{path_working}/train_index.csv\")[\n",
    "        \"train\"\n",
    "    ].values.tolist()\n",
    "    val_index = pd.read_csv(f\"{path_working}/val_index.csv\")[\"val\"].values.tolist()\n",
    "\n",
    "elif cfg[\"mode_split\"] == \"strat\":\n",
    "    train_index = train_index_strat\n",
    "    val_index = val_index_strat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T12:47:13.215326Z",
     "iopub.status.busy": "2024-03-27T12:47:13.215054Z",
     "iopub.status.idle": "2024-03-27T12:47:13.227716Z",
     "shell.execute_reply": "2024-03-27T12:47:13.226776Z",
     "shell.execute_reply.started": "2024-03-27T12:47:13.215304Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# transform\n",
    "transform_train = T.Compose(\n",
    "    [\n",
    "        v2.PILToTensor(),\n",
    "        v2.RandomHorizontalFlip(p=cfg[\"p\"]),\n",
    "        v2.RandomVerticalFlip(p=cfg[\"p\"]),\n",
    "        v2.RandomAffine(\n",
    "            degrees=cfg[\"degrees\"],\n",
    "            translate=cfg[\"translate\"],\n",
    "            scale=cfg[\"scale\"],\n",
    "            fill=cfg[\"fill\"],\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_val = T.Compose(\n",
    "    [\n",
    "        v2.PILToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T12:47:13.229296Z",
     "iopub.status.busy": "2024-03-27T12:47:13.228844Z",
     "iopub.status.idle": "2024-03-27T12:47:20.119120Z",
     "shell.execute_reply": "2024-03-27T12:47:20.118243Z",
     "shell.execute_reply.started": "2024-03-27T12:47:13.229265Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The configuration is:\n",
      "who : baptiste\n",
      "no_wandb : False\n",
      "name_exp : PatientModelCrossAttentionTab - vit_small_patch16_224 - lora\n",
      "lr : 5e-06\n",
      "batch_size : 1\n",
      "nb_epochs : 20\n",
      "timm : True\n",
      "timm_model : vit_small_patch16_224.augreg_in21k\n",
      "dino : False\n",
      "dino_size : vits\n",
      "adapter : lora\n",
      "model_name : PatientModelCrossAttentionTab\n",
      "pretrained : True\n",
      "pretrained_path : \n",
      "nb_class : 2\n",
      "scheduler : None\n",
      "dataset_name : DatasetPerPatient\n",
      "device_1 : cuda:0\n",
      "device_2 : cuda:1\n",
      "filename : /kaggle/working/submission.csv\n",
      "sub_batch_size : 16\n",
      "latent_att : 512\n",
      "head_1 : 8\n",
      "head_2 : 2\n",
      "feature_dim : 384\n",
      "aggregation : avg\n",
      "beta_1 : 0.5\n",
      "beta_2 : 0.9\n",
      "weight_decay : 0.05\n",
      "weight_class_0 : 3.0\n",
      "weight_class_1 : 1.0\n",
      "mask_ratio : 0.75\n",
      "image_size : 224\n",
      "patch_size : 16\n",
      "mae_pretrained : small_testset_800it.pt\n",
      "with_tab : True\n",
      "mode_split : strat\n",
      "degrees : (-5, 5)\n",
      "translate : (0.1, 0.1)\n",
      "scale : (0.9, 1.0)\n",
      "fill : (255, 232, 201)\n",
      "p : 0.1\n",
      "Loading custom model PatientModelCrossAttentionTab\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c5e68b0762a4be2bf5289d4cb169303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/120M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training is from scatch\n",
      "Use lora adapter\n",
      "==================================================\n",
      "The model has 23505374 parameters\n",
      "The model has 1839710 trainable parameters\n",
      "It represents 7.827 % trainable parameters\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data_factory = DataloaderFactory()\n",
    "model_factory = ModelFactory()\n",
    "dataloader_train = data_factory(\n",
    "    cfg,\n",
    "    mode=\"train\",\n",
    "    split_indexes=train_index,\n",
    "    path_root=path_root,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    transform=transform_train,\n",
    "    oversampling={\"0\": 1, \"1\": 1},\n",
    ")\n",
    "\n",
    "dataloader_val = data_factory(\n",
    "    cfg,\n",
    "    mode=\"train\",\n",
    "    split_indexes=val_index,\n",
    "    path_root=path_root,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    transform=transform_val,\n",
    "    oversampling={\"0\": 1, \"1\": 1},\n",
    ")\n",
    "\n",
    "\n",
    "# load model\n",
    "model = model_factory(cfg).to(cfg[\"device_1\"])\n",
    "\n",
    "# optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=cfg[\"lr\"],\n",
    "    betas=(cfg[\"beta_1\"], cfg[\"beta_2\"]),\n",
    "    weight_decay=cfg[\"weight_decay\"],\n",
    ")\n",
    "\n",
    "scheduler = None  # torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg['nb_epochs'], eta_min=5e-6)\n",
    "\n",
    "soft_max = torch.nn.Softmax(1)\n",
    "\n",
    "\n",
    "cfg[\"nb_params_train\"] = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "cfg[\"nb_params_tot\"] = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(f'The model has {cfg[\"nb_params_tot\"]} parameters')\n",
    "print(f'The model has {cfg[\"nb_params_train\"]} trainable parameters')\n",
    "print(\n",
    "    f'It represents {np.round(100 * cfg[\"nb_params_train\"]/cfg[\"nb_params_tot\"], 3)} % trainable parameters'\n",
    ")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T12:47:20.120504Z",
     "iopub.status.busy": "2024-03-27T12:47:20.120210Z",
     "iopub.status.idle": "2024-03-27T12:47:21.261741Z",
     "shell.execute_reply": "2024-03-27T12:47:21.260934Z",
     "shell.execute_reply.started": "2024-03-27T12:47:20.120480Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the entire model\n",
    "if len(cfg[\"mae_pretrained\"]) > 0:\n",
    "    MAE_model = torch.load(os.path.join(path_mae, cfg[\"mae_pretrained\"]))\n",
    "    model.blocks = copy.deepcopy(MAE_model.encoder.model.blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T12:47:21.264705Z",
     "iopub.status.busy": "2024-03-27T12:47:21.264412Z",
     "iopub.status.idle": "2024-03-27T12:47:21.269653Z",
     "shell.execute_reply": "2024-03-27T12:47:21.268785Z",
     "shell.execute_reply.started": "2024-03-27T12:47:21.264682Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "weight = torch.tensor([2.5, 1.0]).to(\"cuda:0\")\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T12:47:21.271635Z",
     "iopub.status.busy": "2024-03-27T12:47:21.271125Z",
     "iopub.status.idle": "2024-03-27T13:23:11.549332Z",
     "shell.execute_reply": "2024-03-27T13:23:11.548262Z",
     "shell.execute_reply.started": "2024-03-27T12:47:21.271602Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbaptcallard\u001b[0m (\u001b[33mii_timm\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20240327_124723-lf9l2ujz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ii_timm/DLMI/runs/lf9l2ujz' target=\"_blank\">PatientModelCrossAttentionTab - vit_small_patch16_224 - lora</a></strong> to <a href='https://wandb.ai/ii_timm/DLMI' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ii_timm/DLMI' target=\"_blank\">https://wandb.ai/ii_timm/DLMI</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ii_timm/DLMI/runs/lf9l2ujz' target=\"_blank\">https://wandb.ai/ii_timm/DLMI/runs/lf9l2ujz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training ...\n",
      "==================================================\n",
      "                Epoch 0\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [02:14<00:00,  1.03s/it]\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2184: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_balance_acc : 0.692308 / train_avg_loss : 0.17662\n",
      "[1] [130]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:22<00:00,  1.45it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2184: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_balance_acc : 0.69697 / val_avg_loss : 0.160592\n",
      "[1] [33]\n",
      "0.16059231633941332 10000\n",
      "Improve avg loss :\n",
      "checkpoint saved to: ./model0_finetune.pt\n",
      "time 0.9652812466299607\n",
      "==================================================\n",
      "                Epoch 1\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [01:29<00:00,  1.46it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2184: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_balance_acc : 0.692308 / train_avg_loss : 0.175508\n",
      "[1] [130]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:11<00:00,  2.81it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2184: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_balance_acc : 0.69697 / val_avg_loss : 0.160385\n",
      "[1] [33]\n",
      "0.16038476472551172 0.16059231633941332\n",
      "Improve avg loss :\n",
      "checkpoint saved to: ./model1_finetune.pt\n",
      "time 0.6218662949427505\n",
      "==================================================\n",
      "                Epoch 2\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [01:28<00:00,  1.48it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2184: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_balance_acc : 0.692308 / train_avg_loss : 0.176352\n",
      "[1] [130]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:11<00:00,  2.94it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2184: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_balance_acc : 0.69697 / val_avg_loss : 0.159445\n",
      "[1] [33]\n",
      "0.159444641999223 0.16038476472551172\n",
      "Improve avg loss :\n",
      "checkpoint saved to: ./model2_finetune.pt\n",
      "time 0.6113074469420076\n",
      "==================================================\n",
      "                Epoch 3\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [01:27<00:00,  1.48it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2184: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_balance_acc : 0.692308 / train_avg_loss : 0.175708\n",
      "[1] [130]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:11<00:00,  2.90it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2184: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_balance_acc : 0.69697 / val_avg_loss : 0.154394\n",
      "[1] [33]\n",
      "0.1543941211068269 0.159444641999223\n",
      "Improve avg loss :\n",
      "checkpoint saved to: ./model3_finetune.pt\n",
      "time 0.6093163753579731\n",
      "==================================================\n",
      "                Epoch 4\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [01:27<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_balance_acc : 0.344961 / train_avg_loss : 0.174251\n",
      "[0 1] [  1 129]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:11<00:00,  2.86it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2184: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_balance_acc : 0.69697 / val_avg_loss : 0.153844\n",
      "[1] [33]\n",
      "0.15384402667934244 0.1543941211068269\n",
      "Improve avg loss :\n",
      "checkpoint saved to: ./model4_finetune.pt\n",
      "time 0.607740064340135\n",
      "==================================================\n",
      "                Epoch 5\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [01:28<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_balance_acc : 0.567734 / train_avg_loss : 0.168104\n",
      "[0 1] [ 14 116]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:11<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_balance_acc : 0.870968 / val_avg_loss : 0.137757\n",
      "[0 1] [ 2 31]\n",
      "0.13775692078651805 0.15384402667934244\n",
      "Improve avg loss :\n",
      "checkpoint saved to: ./model5_finetune.pt\n",
      "time 0.6155322899847674\n",
      "==================================================\n",
      "                Epoch 6\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [01:29<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_balance_acc : 0.590475 / train_avg_loss : 0.163825\n",
      "[0 1] [ 29 101]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:11<00:00,  2.77it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2184: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_balance_acc : 0.69697 / val_avg_loss : 0.137474\n",
      "[1] [33]\n",
      "0.13747364491450065 0.13775692078651805\n",
      "Improve avg loss :\n",
      "checkpoint saved to: ./model6_finetune.pt\n",
      "time 0.6223686633665869\n",
      "==================================================\n",
      "                Epoch 7\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [01:27<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_balance_acc : 0.679242 / train_avg_loss : 0.158613\n",
      "[0 1] [ 29 101]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:11<00:00,  2.86it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2184: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_balance_acc : 0.69697 / val_avg_loss : 0.142136\n",
      "[1] [33]\n",
      "0.1421360850447055 0.13747364491450065\n",
      "time 0.6098025093780705\n",
      "==================================================\n",
      "                Epoch 8\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [01:29<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_balance_acc : 0.69 / train_avg_loss : 0.159351\n",
      "[0 1] [ 30 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:11<00:00,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_balance_acc : 0.8775 / val_avg_loss : 0.116883\n",
      "[0 1] [ 8 25]\n",
      "0.11688325796840769 0.13747364491450065\n",
      "Improve avg loss :\n",
      "checkpoint saved to: ./model8_finetune.pt\n",
      "time 0.61893706526493\n",
      "==================================================\n",
      "                Epoch 9\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [01:30<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_balance_acc : 0.710459 / train_avg_loss : 0.155474\n",
      "[0 1] [32 98]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:11<00:00,  2.77it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2184: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_balance_acc : 0.69697 / val_avg_loss : 0.143872\n",
      "[1] [33]\n",
      "0.1438716763461178 0.11688325796840769\n",
      "time 0.6260636086844228\n",
      "==================================================\n",
      "                Epoch 10\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [01:29<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_balance_acc : 0.719413 / train_avg_loss : 0.151164\n",
      "[0 1] [37 93]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:11<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_balance_acc : 0.896552 / val_avg_loss : 0.112214\n",
      "[0 1] [ 4 29]\n",
      "0.11221387954146574 0.11688325796840769\n",
      "Improve avg loss :\n",
      "checkpoint saved to: ./model10_finetune.pt\n",
      "time 0.6237872012553771\n",
      "==================================================\n",
      "                Epoch 11\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [01:32<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_balance_acc : 0.767435 / train_avg_loss : 0.140053\n",
      "[0 1] [36 94]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:12<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_balance_acc : 0.886364 / val_avg_loss : 0.103265\n",
      "[0 1] [11 22]\n",
      "0.10326516941528428 0.11221387954146574\n",
      "Improve avg loss :\n",
      "checkpoint saved to: ./model11_finetune.pt\n",
      "time 0.6434318770660213\n",
      "==================================================\n",
      "                Epoch 12\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [01:34<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_balance_acc : 0.700523 / train_avg_loss : 0.13576\n",
      "[0 1] [37 93]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:14<00:00,  2.26it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2184: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_balance_acc : 0.69697 / val_avg_loss : 0.142303\n",
      "[1] [33]\n",
      "0.14230306974301735 0.10326516941528428\n",
      "time 0.6683420681514622\n",
      "==================================================\n",
      "                Epoch 13\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [01:35<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_balance_acc : 0.79304 / train_avg_loss : 0.133264\n",
      "[0 1] [39 91]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:11<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_balance_acc : 0.8775 / val_avg_loss : 0.098711\n",
      "[0 1] [ 8 25]\n",
      "0.09871128980409015 0.10326516941528428\n",
      "Improve avg loss :\n",
      "checkpoint saved to: ./model13_finetune.pt\n",
      "time 0.6570844065192287\n",
      "==================================================\n",
      "                Epoch 14\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [01:32<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_balance_acc : 0.765152 / train_avg_loss : 0.136737\n",
      "[0 1] [42 88]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:13<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_balance_acc : 0.8775 / val_avg_loss : 0.093213\n",
      "[0 1] [ 8 25]\n",
      "0.09321277832725283 0.09871128980409015\n",
      "Improve avg loss :\n",
      "checkpoint saved to: ./model14_finetune.pt\n",
      "time 0.6529751555319944\n",
      "==================================================\n",
      "                Epoch 15\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [01:36<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_balance_acc : 0.791366 / train_avg_loss : 0.122195\n",
      "[0 1] [43 87]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:11<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_balance_acc : 0.821154 / val_avg_loss : 0.099593\n",
      "[0 1] [13 20]\n",
      "0.09959263756701892 0.09321277832725283\n",
      "time 0.6668081254315522\n",
      "==================================================\n",
      "                Epoch 16\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [01:32<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_balance_acc : 0.786643 / train_avg_loss : 0.123956\n",
      "[0 1] [36 94]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:11<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_balance_acc : 0.886364 / val_avg_loss : 0.087496\n",
      "[0 1] [11 22]\n",
      "0.08749590593982827 0.09321277832725283\n",
      "Improve avg loss :\n",
      "checkpoint saved to: ./model16_finetune.pt\n",
      "time 0.6405976740129155\n",
      "==================================================\n",
      "                Epoch 17\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [01:32<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_balance_acc : 0.803204 / train_avg_loss : 0.120685\n",
      "[0 1] [38 92]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:11<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_balance_acc : 0.821154 / val_avg_loss : 0.094762\n",
      "[0 1] [13 20]\n",
      "0.09476168512959372 0.08749590593982827\n",
      "time 0.6383592906905098\n",
      "==================================================\n",
      "                Epoch 18\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [01:30<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_balance_acc : 0.789522 / train_avg_loss : 0.11643\n",
      "[0 1] [34 96]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:11<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_balance_acc : 0.821154 / val_avg_loss : 0.095105\n",
      "[0 1] [13 20]\n",
      "0.09510490696199915 0.08749590593982827\n",
      "time 0.6275821858388515\n",
      "==================================================\n",
      "                Epoch 19\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [01:33<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_balance_acc : 0.808741 / train_avg_loss : 0.118781\n",
      "[0 1] [43 87]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:11<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_balance_acc : 0.85119 / val_avg_loss : 0.087469\n",
      "[0 1] [12 21]\n",
      "0.08746904189783064 0.08749590593982827\n",
      "Improve avg loss :\n",
      "checkpoint saved to: ./model19_finetune.pt\n",
      "time 0.6508636723266789\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "###              Training\n",
    "#############################################\n",
    "\n",
    "best_loss = 10000\n",
    "\n",
    "if not cfg[\"no_wandb\"]:\n",
    "    run = wandb.init(\n",
    "        project=\"DLMI\",\n",
    "        entity=\"ii_timm\",\n",
    "        name=cfg[\"name_exp\"],\n",
    "        config=cfg,\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Start Training ...\")\n",
    "for epoch in range(cfg[\"nb_epochs\"]):\n",
    "    model.train()\n",
    "    print(\"=\" * 50)\n",
    "    print(\" \" * 15, f\"Epoch {epoch}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    train_cum_loss = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    #############################\n",
    "    ###     VAL loop\n",
    "    #############################\n",
    "    train_pred = []\n",
    "    train_label = []\n",
    "\n",
    "    for x, annotation in tqdm(dataloader_train):\n",
    "        optimizer.zero_grad()\n",
    "        x = x.to(cfg[\"device_1\"]).squeeze(0)\n",
    "\n",
    "        if cfg[\"with_tab\"]:\n",
    "            # define tabular data\n",
    "            lymph_count, age, bin_gender = (\n",
    "                annotation[\"LYMPH_COUNT\"],\n",
    "                annotation[\"AGE\"],\n",
    "                annotation[\"BIN_GENDER\"],\n",
    "            )\n",
    "            x_tab = torch.zeros((1, 4)).to(cfg[\"device_1\"])\n",
    "            x_tab[0, int(bin_gender)] = 1\n",
    "            x_tab[0, 2] = torch.clamp(age + 1e-4 * np.random.rand(1)[0], 0, 1)\n",
    "            x_tab[0, 3] = torch.clamp(lymph_count + 1e-4 * np.random.rand(1)[0], 0, 1)\n",
    "\n",
    "            xout_sub_batch = model(x, x_tab, \"train\")\n",
    "        else:\n",
    "            # None tabular data\n",
    "            xout_sub_batch = model(x, \"train\")\n",
    "\n",
    "        # compute the loss and pred\n",
    "        loss = loss_fn(\n",
    "            xout_sub_batch.unsqueeze(0), annotation[\"LABEL\"].to(cfg[\"device_1\"])\n",
    "        ) / (x.shape[0] / cfg[\"sub_batch_size\"])\n",
    "        pred = torch.argmax(soft_max(xout_sub_batch.unsqueeze(0)), dim=1)\n",
    "        train_cum_loss += loss.item()\n",
    "\n",
    "        # store the res.\n",
    "        train_pred.extend(pred.detach().cpu().tolist())\n",
    "        train_label.extend(annotation[\"LABEL\"].detach().cpu().tolist())\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "    # compute agg. scores\n",
    "    train_balance_acc = balanced_accuracy_score(train_pred, train_label)\n",
    "    train_avg_loss = train_cum_loss / len(dataloader_train)\n",
    "    print(\n",
    "        f\"train_balance_acc : {np.round(train_balance_acc, 6)} / train_avg_loss : {np.round(train_avg_loss, 6)}\"\n",
    "    )\n",
    "    unique_train, count_train = np.unique(train_pred, return_counts=True)\n",
    "    print(unique_train, count_train)\n",
    "\n",
    "    # edge case\n",
    "    if len(unique_train) == 1:\n",
    "        if unique_train[0] == 1:\n",
    "            count_train = count_train.tolist()\n",
    "            count_train.insert(0, 0)\n",
    "        if unique_train[0] == 0:\n",
    "            count_train = count_train.tolist()\n",
    "            count_train.append(0)\n",
    "\n",
    "    #############################\n",
    "    ###     VAL loop\n",
    "    #############################\n",
    "    val_pred = []\n",
    "    val_label = []\n",
    "    val_cum_loss = 0\n",
    "    model.eval()\n",
    "    for x, annotation in tqdm(dataloader_val):\n",
    "        # forward\n",
    "        x = x.to(cfg[\"device_1\"]).squeeze(0)\n",
    "\n",
    "        if cfg[\"with_tab\"]:\n",
    "            # define tabular data\n",
    "            lymph_count, age, bin_gender = (\n",
    "                annotation[\"LYMPH_COUNT\"],\n",
    "                annotation[\"AGE\"],\n",
    "                annotation[\"BIN_GENDER\"],\n",
    "            )\n",
    "            x_tab = torch.zeros((1, 4)).to(cfg[\"device_1\"])\n",
    "            x_tab[0, int(bin_gender)] = 1\n",
    "            x_tab[0, 2] = age\n",
    "            x_tab[0, 3] = lymph_count\n",
    "\n",
    "            xout_sub_batch = model(x, x_tab, \"val\")\n",
    "        else:\n",
    "            xout_sub_batch = model(x, \"val\")\n",
    "        # compute loss\n",
    "        loss = loss_fn(\n",
    "            xout_sub_batch.unsqueeze(0), annotation[\"LABEL\"].to(cfg[\"device_1\"])\n",
    "        ) / (x.shape[0] / cfg[\"sub_batch_size\"])\n",
    "        pred = torch.argmax(soft_max(xout_sub_batch.unsqueeze(0)), dim=1)\n",
    "        val_cum_loss += loss.item()\n",
    "        val_pred.extend(pred.detach().cpu().tolist())\n",
    "        val_label.extend(annotation[\"LABEL\"].detach().cpu().tolist())\n",
    "\n",
    "    # compute agg. scores\n",
    "    val_balance_acc = balanced_accuracy_score(val_pred, val_label)\n",
    "    val_avg_loss = val_cum_loss / len(dataloader_val)\n",
    "    print(\n",
    "        f\"val_balance_acc : {np.round(val_balance_acc, 6)} / val_avg_loss : {np.round(val_avg_loss, 6)}\"\n",
    "    )\n",
    "    unique_val, count_val = np.unique(val_pred, return_counts=True)\n",
    "    print(unique_val, count_val)\n",
    "\n",
    "    # edge case\n",
    "    if len(unique_val) == 1:\n",
    "        if unique_val[0] == 1:\n",
    "            count_val = count_val.tolist()\n",
    "            count_val.insert(0, 0)\n",
    "        if unique_val[0] == 0:\n",
    "            count_val = count_val.tolist()\n",
    "            count_val.append(0)\n",
    "    print(val_avg_loss, best_loss)\n",
    "\n",
    "    # Save best model + prints\n",
    "    if val_avg_loss < best_loss:\n",
    "        best_loss = val_avg_loss\n",
    "\n",
    "        print(\"Improve avg loss :\")\n",
    "        save_path_finetune = os.path.join(\"./\", \"model\" + str(epoch) + \"_finetune.pt\")\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"scheduler_state_dict\": scheduler.state_dict()\n",
    "                if cfg[\"scheduler\"] is not None\n",
    "                else None,\n",
    "            },\n",
    "            save_path_finetune,\n",
    "        )\n",
    "        print(\"checkpoint saved to: {}\".format(save_path_finetune))\n",
    "\n",
    "    print(\n",
    "        \"time\",\n",
    "        (time.time() - start_time) / (len(dataloader_val) + len(dataloader_train)),\n",
    "    )\n",
    "\n",
    "    # Save in Wandb\n",
    "    if not cfg[\"no_wandb\"]:\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"balance_acc/train\": train_balance_acc,\n",
    "                \"loss/train\": train_avg_loss,\n",
    "                \"balance_acc/val\": val_balance_acc,\n",
    "                \"loss/val\": val_avg_loss,\n",
    "                \"time\": (time.time() - start_time)\n",
    "                / (len(dataloader_val) + len(dataloader_train)),\n",
    "                \"count_train_0\": count_train[0],\n",
    "                \"count_train_1\": count_train[1],\n",
    "                \"count_val_0\": count_val[0],\n",
    "                \"count_val_1\": count_val[1],\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T13:23:11.551130Z",
     "iopub.status.busy": "2024-03-27T13:23:11.550750Z",
     "iopub.status.idle": "2024-03-27T13:23:13.828757Z",
     "shell.execute_reply": "2024-03-27T13:23:13.827668Z",
     "shell.execute_reply.started": "2024-03-27T13:23:11.551096Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if not cfg[\"no_wandb\"]:\n",
    "    model_artifact = wandb.Artifact(\n",
    "        \"model\" + str(uuid.uuid1()).replace(\"-\", \"\"), type=\"model\"\n",
    "    )\n",
    "    model_artifact.add_file(save_path_finetune)\n",
    "    wandb.log_artifact(model_artifact)\n",
    "\n",
    "    description_artifact = wandb.Artifact(\n",
    "        \"description_model\" + str(uuid.uuid1()).replace(\"-\", \"\"), type=\"python\"\n",
    "    )\n",
    "\n",
    "    !cp -r $path_working/dlmi/src/* $path_working/\n",
    "    description_artifact.add_file(f\"{path_working}/model.py\")\n",
    "    description_artifact.add_file(f\"{path_working}/utils.py\")\n",
    "    description_artifact.add_file(f\"{path_working}/data.py\")\n",
    "    wandb.log_artifact(description_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T13:23:13.830516Z",
     "iopub.status.busy": "2024-03-27T13:23:13.830216Z",
     "iopub.status.idle": "2024-03-27T13:23:14.796266Z",
     "shell.execute_reply": "2024-03-27T13:23:14.795175Z",
     "shell.execute_reply.started": "2024-03-27T13:23:13.830487Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataloader_test = data_factory(\n",
    "    cfg,\n",
    "    mode=\"test\",\n",
    "    split_indexes=test_index,\n",
    "    path_root=path_root,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    transform=transform_val,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T13:23:14.797879Z",
     "iopub.status.busy": "2024-03-27T13:23:14.797589Z",
     "iopub.status.idle": "2024-03-27T13:23:41.874076Z",
     "shell.execute_reply": "2024-03-27T13:23:41.872129Z",
     "shell.execute_reply.started": "2024-03-27T13:23:14.797854Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model ./model19_finetune.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:26<00:00,  1.56it/s]\n"
     ]
    }
   ],
   "source": [
    "test_pred = []\n",
    "test_ID = []\n",
    "map_results = {\n",
    "    \"Id\": [],\n",
    "    \"Predicted\": [],\n",
    "}\n",
    "\n",
    "map_results_logit = {\n",
    "    \"Id\": [],\n",
    "    \"logit_0\": [],\n",
    "    \"logit_1\": [],\n",
    "}\n",
    "\n",
    "print(\"Load model\", save_path_finetune)\n",
    "model.load_state_dict(torch.load(save_path_finetune)[\"model_state_dict\"])\n",
    "model.eval()\n",
    "for x, annotation in tqdm(dataloader_test):\n",
    "    # forward\n",
    "    x = x.to(cfg[\"device_1\"])\n",
    "    with torch.no_grad():\n",
    "        x = x.to(cfg[\"device_1\"]).squeeze(0)\n",
    "\n",
    "        if cfg[\"with_tab\"]:\n",
    "            # define tabular data\n",
    "            lymph_count, age, bin_gender = (\n",
    "                annotation[\"LYMPH_COUNT\"],\n",
    "                annotation[\"AGE\"],\n",
    "                annotation[\"BIN_GENDER\"],\n",
    "            )\n",
    "            x_tab = torch.zeros((1, 4)).to(cfg[\"device_1\"])\n",
    "            x_tab[0, int(bin_gender)] = 1\n",
    "            x_tab[0, 2] = age\n",
    "            x_tab[0, 3] = lymph_count\n",
    "\n",
    "            x = model(x, x_tab, \"val\")\n",
    "        else:\n",
    "            x = model(x, \"val\")\n",
    "\n",
    "        logit = soft_max(x.unsqueeze(0))\n",
    "        pred = torch.argmax(logit, dim=1)\n",
    "\n",
    "        map_results[\"Predicted\"].extend(pred.detach().cpu().tolist())\n",
    "        map_results[\"Id\"].extend(annotation[\"ID\"])\n",
    "\n",
    "        map_results_logit[\"logit_0\"].append(logit[0][0].item())\n",
    "        map_results_logit[\"logit_1\"].append(logit[0][1].item())\n",
    "        map_results_logit[\"Id\"].extend(annotation[\"ID\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save in Wandb !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T13:23:41.875936Z",
     "iopub.status.busy": "2024-03-27T13:23:41.875553Z",
     "iopub.status.idle": "2024-03-27T13:23:54.518671Z",
     "shell.execute_reply": "2024-03-27T13:23:54.517925Z",
     "shell.execute_reply.started": "2024-03-27T13:23:41.875902Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='185.151 MB of 185.151 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>balance_acc/train</td><td>▆▆▆▆▁▄▅▆▆▇▇▇▆█▇█████</td></tr><tr><td>balance_acc/val</td><td>▁▁▁▁▁▇▁▁▇▁██▁▇▇▅█▅▅▆</td></tr><tr><td>count_train_0</td><td>▁▁▁▁▁▃▆▆▆▆▇▇▇▇██▇▇▇█</td></tr><tr><td>count_train_1</td><td>█████▆▃▃▃▃▂▂▂▂▁▁▂▂▂▁</td></tr><tr><td>count_val_0</td><td>▁▁▁▁▁▂▁▁▅▁▃▇▁▅▅█▇██▇</td></tr><tr><td>count_val_1</td><td>█████▇██▄█▆▂█▄▄▁▂▁▁▂</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>loss/train</td><td>█████▇▇▆▆▆▅▄▃▃▃▂▂▁▁▁</td></tr><tr><td>loss/val</td><td>███▇▇▆▆▆▄▆▃▃▆▂▂▂▁▂▂▁</td></tr><tr><td>time</td><td>█▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>balance_acc/train</td><td>0.80874</td></tr><tr><td>balance_acc/val</td><td>0.85119</td></tr><tr><td>count_train_0</td><td>43</td></tr><tr><td>count_train_1</td><td>87</td></tr><tr><td>count_val_0</td><td>12</td></tr><tr><td>count_val_1</td><td>21</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>loss/train</td><td>0.11878</td></tr><tr><td>loss/val</td><td>0.08747</td></tr><tr><td>time</td><td>0.65087</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">PatientModelCrossAttentionTab - vit_small_patch16_224 - lora</strong> at: <a href='https://wandb.ai/ii_timm/DLMI/runs/lf9l2ujz' target=\"_blank\">https://wandb.ai/ii_timm/DLMI/runs/lf9l2ujz</a><br/>Synced 6 W&B file(s), 0 media file(s), 8 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240327_124723-lf9l2ujz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_results = pd.DataFrame(map_results)\n",
    "df_results.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "df_results = pd.DataFrame(map_results_logit)\n",
    "df_results.to_csv(\"logit.csv\", index=False)\n",
    "\n",
    "if not cfg[\"no_wandb\"]:\n",
    "    # log index\n",
    "    df_train_index = pd.DataFrame({\"train\": train_index})\n",
    "    df_train_index.to_csv(\"train_index.csv\", index=False)\n",
    "    df_val_index = pd.DataFrame({\"val\": val_index})\n",
    "    df_val_index.to_csv(\"val_index.csv\", index=False)\n",
    "\n",
    "    csv_artifact = wandb.Artifact(\n",
    "        \"submission\" + str(uuid.uuid1()).replace(\"-\", \"\"), type=\"csv\"\n",
    "    )\n",
    "    csv_artifact.add_file(\"submission.csv\")\n",
    "    csv_artifact.add_file(\"logit.csv\")\n",
    "    csv_artifact.add_file(\"train_index.csv\")\n",
    "    csv_artifact.add_file(\"val_index.csv\")\n",
    "    wandb.log_artifact(csv_artifact)\n",
    "\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4453724,
     "sourceId": 7641601,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4680124,
     "sourceId": 7956772,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
